{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pynbody\n",
    "import numpy as np\n",
    "import os, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pynbody import util, units, family\n",
    "from pyspark.sql.types import ArrayType, DoubleType, FloatType, NullType, StructType, StructField, StringType, ByteType\n",
    "\n",
    "# set up config directory\n",
    "os.environ['SPARK_CONF_DIR'] = os.path.realpath('./spark_config')\n",
    "\n",
    "# set up the submit arguments\n",
    "files = \"--files %s/metrics.properties\"%os.environ['SPARK_CONF_DIR']\n",
    "packages = \"--packages com.databricks:spark-csv_2.10:1.3.0\"\n",
    "shell = \"pyspark-shell\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \" \".join([files,packages,shell])\n",
    "\n",
    "# how many cores do we have for the driver\n",
    "ncores = int(os.environ.get('LSB_DJOB_NUMPROC', 1)) \n",
    "\n",
    "# here we set the memory we want spark to use for the driver JVM\n",
    "#os.environ['SPARK_DRIVER_MEMORY'] = '%dG'%(ncores*2*0.7)\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '2g'\n",
    "# we have to tell spark which python executable we are using\n",
    "os.environ['PYSPARK_PYTHON'] = subprocess.check_output('which python', shell=True).rstrip()\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "exec_cores = 4\n",
    "num_execs = 100\n",
    "\n",
    "conf.set('spark.executor.instances', str(num_execs))\n",
    "conf.set('spark.executor.cores', str(exec_cores))\n",
    "\n",
    "try: \n",
    "    sc.stop()\n",
    "except: \n",
    "    pass\n",
    "sc = SparkContext(master='yarn-client', conf=conf)\n",
    "sqc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ugly code for reading in the particles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batches(n_parts, n_batches): \n",
    "    batch_size = n_parts/n_batches\n",
    "    curr_start = 0\n",
    "    while curr_start < n_parts:\n",
    "        yield((curr_start, min(curr_start+batch_size, n_parts+1)))\n",
    "        curr_start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class SparkTipsySnap(pynbody.tipsy.TipsySnap) :\n",
    "    \n",
    "    def _load_main_file(self):\n",
    "        logger.info(\"Loading data from main file %s\", self._filename)\n",
    "\n",
    "        f = util.open_(self._filename, 'rb')\n",
    "        f.seek(32)\n",
    "\n",
    "        max_item_size = max(\n",
    "            [q.itemsize for q in self._g_dtype, self._d_dtype, self._s_dtype])\n",
    "        tbuf = bytearray(max_item_size * 10240)\n",
    "\n",
    "        for fam, dtype in ((family.gas, self._g_dtype), (family.dm, self._d_dtype), (family.star, self._s_dtype)):\n",
    "            self_fam = self[fam]\n",
    "            st_len = dtype.itemsize\n",
    "            for readlen, buf_index, mem_index in self._load_control.iterate([fam], [fam], multiskip=True):\n",
    "                # Read in the block\n",
    "                if mem_index is None:\n",
    "                    f.seek(st_len * readlen, 1)\n",
    "                    continue\n",
    "\n",
    "                buf = np.fromstring(f.read(st_len * readlen), dtype=dtype)\n",
    "                \n",
    "                if self._byteswap:\n",
    "                    buf = buf.byteswap()\n",
    "                \n",
    "                yield buf, buf_index, fam\n",
    "                \n",
    "    \n",
    "fam_lookup = {family.dm:'d', family.gas: 'g', family.star:'s'}\n",
    "\n",
    "def buf_to_row(buf, buf_index, names, fam) : \n",
    "    if type(buf_index) == slice:\n",
    "        buf_index = xrange(buf_index.start, buf_index.stop)\n",
    "    for i in buf_index :\n",
    "        d = {name:float(buf[name][i]) for name in buf.dtype.names} # this should be an OrderedDict\n",
    "        for name in names: \n",
    "            if name not in d: \n",
    "                d[name] = np.nan\n",
    "                d['fam'] = fam_lookup[fam]\n",
    "        yield Row(**d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_partition(filename, batch_iter, names) : \n",
    "    for batch in batch_iter :\n",
    "        s = SparkTipsySnap(filename, take = xrange(*batch))\n",
    "        loader = s._load_main_file()\n",
    "        for buf, bi, fam in loader : \n",
    "            for row in buf_to_row(buf, bi, names, fam) : \n",
    "                yield row\n",
    "            del(buf)\n",
    "        del(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data read and conversion to `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/cluster/home03/sdid/roskarr/work/testing/cosmo25cmb.768g2_dm.001024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = SparkTipsySnap(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = set(s._g_dtype.names) | set(s._s_dtype.names) | set(s._d_dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_parts = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = make_batches(n_parts, 800-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches_rdd = sc.parallelize(batches, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.addPyFile('/cluster/home03/sdid/roskarr/src/pynbody/dist/pynbody-0.31-py2.7-linux-x86_64.egg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_rdd = batches_rdd.mapPartitions(lambda iterator: load_partition(filename, iterator, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields = [StructField('eps', FloatType(), True), \n",
    "                               StructField('fam', StringType(), True),\n",
    "                               StructField('mass', FloatType(), True),\n",
    "                               StructField('metals', FloatType(), True),\n",
    "                               StructField('phi', FloatType(), True),\n",
    "                              StructField('rho', FloatType(), True),\n",
    "                              StructField('temp', FloatType(), True),\n",
    "                              StructField('tform', FloatType(), True),\n",
    "                              StructField('vx', FloatType(), True),\n",
    "                              StructField('vy', FloatType(), True),\n",
    "                              StructField('vz', FloatType(), True),\n",
    "                              StructField('x', FloatType(), True),\n",
    "                              StructField('y', FloatType(), True),\n",
    "                              StructField('z', FloatType(), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_new = StructType(fields = [StructField('mass', FloatType(), True), \n",
    "                                  StructField('x', FloatType(), True),\n",
    "                                  StructField('y', FloatType(), True),\n",
    "                                  StructField('z', FloatType(), True),\n",
    "                                  StructField('vx', FloatType(), True),\n",
    "                                  StructField('vy', FloatType(), True),\n",
    "                                  StructField('vz', FloatType(), True),\n",
    "                                  StructField('eps', FloatType(), True),\n",
    "                                  StructField('phi', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(eps=1.3799999578623101e-05, fam='g', mass=1.0154856816546598e-10, metals=0.0, phi=0.099265918135643, rho=0.0, temp=500.0, tform=nan, vx=0.14497043192386627, vy=-0.048239488154649734, vz=-0.08926118165254593, x=-0.41583994030952454, y=-0.4288076162338257, z=0.42203330993652344)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sim_rdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+----------+\n",
      "|         mass|          x|          y|         z|\n",
      "+-------------+-----------+-----------+----------+\n",
      "|1.0154857E-10|-0.41583994|-0.42880762| 0.4220333|\n",
      "|1.0154857E-10| -0.4170401| -0.4275199|0.42231753|\n",
      "|1.0154857E-10|-0.41655722|-0.42766565| 0.4223259|\n",
      "|1.0154857E-10|-0.41671997|-0.42697328| 0.4228194|\n",
      "|1.0154857E-10| -0.4170576|-0.42447266| 0.4226917|\n",
      "|1.0154857E-10|-0.41692457| -0.4255475|0.42324886|\n",
      "|1.0154857E-10|-0.41573152| -0.4264287|0.42298603|\n",
      "|1.0154857E-10|-0.41570926|-0.42558408|0.42263356|\n",
      "|1.0154857E-10|-0.41670665|-0.42773277| 0.4234212|\n",
      "|1.0154857E-10| -0.4164624|-0.42720002| 0.4232937|\n",
      "|1.0154857E-10|-0.41709548|-0.42287406|0.42332387|\n",
      "|1.0154857E-10|-0.41609466|-0.42538744|0.42336327|\n",
      "|1.0154857E-10|-0.41849965|-0.42489263|0.42386532|\n",
      "|1.0154857E-10|-0.41691625|-0.42734033|0.42359728|\n",
      "|1.0154857E-10|-0.41441837|-0.43146068|0.42128232|\n",
      "|1.0154857E-10|-0.41666746|-0.42466545|0.42309728|\n",
      "|1.0154857E-10|-0.41687754| -0.4288358| 0.4233506|\n",
      "|1.0154857E-10| -0.4187097|-0.42451838|0.42291296|\n",
      "|1.0154857E-10|-0.41178206|-0.43233752|0.42118406|\n",
      "|1.0154857E-10|-0.41555837| -0.4293461|0.42326656|\n",
      "+-------------+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('mass', 'x', 'y', 'z').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981808640"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('/user/roskarr/nbody/cosmo25cmb.768g2_dm/cosmo25cmb.768g2_dm.001024.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the full dataset from a distributed parquet file\n",
    "\n",
    "Reading in from parquet is now much faster because we don't have to convert and format the data... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 6 ms, total: 10 ms\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = sqc.read.parquet('/user/roskarr/nbody/cosmo25cmb.768g2_dm/cosmo25cmb.768g2_dm.001024.parquet')\n",
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eps: float (nullable = true)\n",
      " |-- fam: string (nullable = true)\n",
      " |-- mass: float (nullable = true)\n",
      " |-- metals: float (nullable = true)\n",
      " |-- phi: float (nullable = true)\n",
      " |-- rho: float (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- tform: float (nullable = true)\n",
      " |-- vx: float (nullable = true)\n",
      " |-- vy: float (nullable = true)\n",
      " |-- vz: float (nullable = true)\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- z: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ms, sys: 0 ns, total: 2 ms\n",
      "Wall time: 2.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1981808640"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling for visualization\n",
    "\n",
    "To visualize the data, it must be brought to the driver and therefore sub-sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled = df.select('x','y','z').sample(False, 0.0001, 1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(sampled['x'], sampled['y'], '.', alpha = .02)\n",
    "plt.xlim(-.5,.5); plt.ylim(-.5,.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a filtered dataset \n",
    "\n",
    "Lets say we want to focus on the big blob around `x = 0` and `y = 0.3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filt_string = 'abs(x)< cast(.1 as float) and y < cast(-.2 as float) and y > cast(-.4 as float)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered = df.filter(filt_string)\n",
    "print '%e'%filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled = filtered.select('x','y').sample(False, 0.001, 1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(sampled['x'], sampled['y'], '.', alpha = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = filtered.select('x', 'y', 'z', 'vx', 'vy', 'vz').rdd.map(lambda r: np.array(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data read using filtering at the source\n",
    "\n",
    "Because each parquet file keeps metadata about data ranges of each column, many files can be completely skipped in some cases, resulting in much more efficient data-ingestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = sqc.read.parquet('/user/roskarr/nbody/cosmo25cmb.768g2_dm/cosmo25cmb.768g2_dm.001024.parquet')\n",
    "filtered2 = df2.filter(filt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is faster than what was done above, i.e. filtering the full cached dataset. In the previous case, every element of the dataset still needed to be touched, whereas in this case large chunks of the data were automatically excluded completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = filtered2.select('x', 'y', 'z', 'vx', 'vy', 'vz').rdd.map(lambda r: np.array(r))\n",
    "points.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model on the data\n",
    "\n",
    "For fun, lets train a K-Means model on the position and velocity data in the filtered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KMeans.train(points, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_rdd = (points.sample(False, 0.1, 1)\n",
    "                     .map(lambda vec: (model.predict(vec),vec))\n",
    "                     .filter(lambda (cluster, vec): cluster < 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_subsample = cluster_rdd.takeSample(False, 100000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [u'#a6cee3', u'#1f78b4', u'#b2df8a', u'#33a02c', u'#fb9a99', u'#e31a1c', u'#fdbf6f', u'#ff7f00', u'#cab2d6', u'#6a3d9a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "cc = ColorConverter()\n",
    "\n",
    "colors_k100 = [cc.to_rgba(colors[cluster]) for (cluster, vec) in cluster_subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = np.vstack((vec for (cluster,vec) in cluster_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(vecs[:,0], vecs[:,1], c= colors_k100, alpha=0.2)\n",
    "plt.xlim(-.1,.1); plt.ylim(-.4,-.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
